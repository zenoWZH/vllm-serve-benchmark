WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 23:20:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
Namespace(backend='vllm', base_url='https://vgpu-test-10.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-2.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-11.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-15.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-7.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-6.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-13.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-14.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-12.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-1.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-8.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-3.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-4.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-5.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-16.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-9.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
2024-10-03 23:37:35 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:25:12 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:35:31 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:38:50 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:40:16 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:55:54 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40947     
Benchmark duration (s):                  5898.64   
Total input tokens:                      9097765   
Total generated tokens:                  7566920   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1542.35   
Output token throughput (tok/s):         1282.82   
---------------Time to First Token----------------
Mean TTFT (ms):                          341.61    
Median TTFT (ms):                        296.62    
P99 TTFT (ms):                           950.00    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          141.67    
Median TPOT (ms):                        139.44    
P99 TPOT (ms):                           247.14    
---------------Inter-token Latency----------------
Mean ITL (ms):                           135.43    
Median ITL (ms):                         67.60     
P99 ITL (ms):                            626.27    
==================================================
2024-10-03 23:32:29 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:48:15 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:55:42 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:55:51 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:59:29 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:02:42 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:27:26 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:51:11 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:56:08 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:56:38 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40945     
Benchmark duration (s):                  5898.75   
Total input tokens:                      9096120   
Total generated tokens:                  7570495   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1542.04   
Output token throughput (tok/s):         1283.41   
---------------Time to First Token----------------
Mean TTFT (ms):                          327.87    
Median TTFT (ms):                        271.98    
P99 TTFT (ms):                           836.24    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          143.20    
Median TPOT (ms):                        140.36    
P99 TPOT (ms):                           246.20    
---------------Inter-token Latency----------------
Mean ITL (ms):                           137.89    
Median ITL (ms):                         67.15     
P99 ITL (ms):                            629.10    
==================================================
2024-10-03 23:39:19 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:45:44 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:53:57 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:19:48 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:44:45 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40947     
Benchmark duration (s):                  5898.96   
Total input tokens:                      9097636   
Total generated tokens:                  7565329   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1542.24   
Output token throughput (tok/s):         1282.49   
---------------Time to First Token----------------
Mean TTFT (ms):                          315.84    
Median TTFT (ms):                        271.33    
P99 TTFT (ms):                           792.99    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          145.26    
Median TPOT (ms):                        142.21    
P99 TPOT (ms):                           247.88    
---------------Inter-token Latency----------------
Mean ITL (ms):                           139.91    
Median ITL (ms):                         70.65     
P99 ITL (ms):                            633.32    
==================================================
2024-10-03 23:24:02 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:02:23 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:52:00 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40949     
Benchmark duration (s):                  5899.27   
Total input tokens:                      9097680   
Total generated tokens:                  7576788   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1542.17   
Output token throughput (tok/s):         1284.36   
---------------Time to First Token----------------
Mean TTFT (ms):                          431.14    
Median TTFT (ms):                        402.77    
P99 TTFT (ms):                           1134.79   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          152.95    
Median TPOT (ms):                        149.28    
P99 TPOT (ms):                           269.72    
---------------Inter-token Latency----------------
Mean ITL (ms):                           145.82    
Median ITL (ms):                         73.93     
P99 ITL (ms):                            684.05    
==================================================
2024-10-03 23:35:57 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:36:58 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:59:44 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:22:02 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:27:46 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:41:26 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40949     
Benchmark duration (s):                  5899.88   
Total input tokens:                      9097447   
Total generated tokens:                  7571834   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.97   
Output token throughput (tok/s):         1283.39   
---------------Time to First Token----------------
Mean TTFT (ms):                          322.43    
Median TTFT (ms):                        279.33    
P99 TTFT (ms):                           806.40    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          145.44    
Median TPOT (ms):                        142.26    
P99 TPOT (ms):                           248.72    
---------------Inter-token Latency----------------
Mean ITL (ms):                           139.94    
Median ITL (ms):                         70.53     
P99 ITL (ms):                            637.84    
==================================================
2024-10-03 23:34:11 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:35:56 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:40:37 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:42:10 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:48:48 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:57:07 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:33:14 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:33:55 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:51:29 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40947     
Benchmark duration (s):                  5900.40   
Total input tokens:                      9098215   
Total generated tokens:                  7574460   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.96   
Output token throughput (tok/s):         1283.72   
---------------Time to First Token----------------
Mean TTFT (ms):                          390.66    
Median TTFT (ms):                        353.16    
P99 TTFT (ms):                           1050.00   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          140.27    
Median TPOT (ms):                        136.29    
P99 TPOT (ms):                           249.00    
---------------Inter-token Latency----------------
Mean ITL (ms):                           133.17    
Median ITL (ms):                         66.59     
P99 ITL (ms):                            629.73    
==================================================
2024-10-04 00:26:55 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:29:00 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:34:47 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:34:54 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:55:42 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40944     
Benchmark duration (s):                  5900.69   
Total input tokens:                      9095893   
Total generated tokens:                  7572154   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.50   
Output token throughput (tok/s):         1283.27   
---------------Time to First Token----------------
Mean TTFT (ms):                          313.12    
Median TTFT (ms):                        269.28    
P99 TTFT (ms):                           796.30    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          145.98    
Median TPOT (ms):                        143.69    
P99 TPOT (ms):                           245.85    
---------------Inter-token Latency----------------
Mean ITL (ms):                           140.82    
Median ITL (ms):                         70.64     
P99 ITL (ms):                            638.19    
==================================================
2024-10-03 23:33:40 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:43:26 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:19:05 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:40:57 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:43:13 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:46:27 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:47:35 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:47:43 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:48:34 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40946     
Benchmark duration (s):                  5901.19   
Total input tokens:                      9097158   
Total generated tokens:                  7572683   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.58   
Output token throughput (tok/s):         1283.25   
---------------Time to First Token----------------
Mean TTFT (ms):                          393.78    
Median TTFT (ms):                        351.45    
P99 TTFT (ms):                           1051.90   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          148.94    
Median TPOT (ms):                        145.25    
P99 TPOT (ms):                           259.26    
---------------Inter-token Latency----------------
Mean ITL (ms):                           142.19    
Median ITL (ms):                         69.11     
P99 ITL (ms):                            674.54    
==================================================
2024-10-03 23:37:04 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:49:30 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40950     
Benchmark duration (s):                  5902.24   
Total input tokens:                      9097026   
Total generated tokens:                  7572466   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.28   
Output token throughput (tok/s):         1282.98   
---------------Time to First Token----------------
Mean TTFT (ms):                          321.27    
Median TTFT (ms):                        277.58    
P99 TTFT (ms):                           837.96    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          148.02    
Median TPOT (ms):                        145.11    
P99 TPOT (ms):                           249.95    
---------------Inter-token Latency----------------
Mean ITL (ms):                           142.71    
Median ITL (ms):                         72.57     
P99 ITL (ms):                            635.52    
==================================================
2024-10-03 23:29:37 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:53:49 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:05:21 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:15:52 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:18:12 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:21:02 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:28:58 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:53:09 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:57:15 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:57:45 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40943     
Benchmark duration (s):                  5902.18   
Total input tokens:                      9096346   
Total generated tokens:                  7568815   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.18   
Output token throughput (tok/s):         1282.38   
---------------Time to First Token----------------
Mean TTFT (ms):                          406.10    
Median TTFT (ms):                        365.87    
P99 TTFT (ms):                           1092.26   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          148.16    
Median TPOT (ms):                        144.02    
P99 TPOT (ms):                           258.60    
---------------Inter-token Latency----------------
Mean ITL (ms):                           141.08    
Median ITL (ms):                         69.08     
P99 ITL (ms):                            668.53    
==================================================
2024-10-03 23:29:35 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:51:06 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:09:43 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:20:52 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40950     
Benchmark duration (s):                  5903.05   
Total input tokens:                      9097902   
Total generated tokens:                  7569779   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.22   
Output token throughput (tok/s):         1282.35   
---------------Time to First Token----------------
Mean TTFT (ms):                          360.19    
Median TTFT (ms):                        316.40    
P99 TTFT (ms):                           1017.27   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          146.96    
Median TPOT (ms):                        144.16    
P99 TPOT (ms):                           255.78    
---------------Inter-token Latency----------------
Mean ITL (ms):                           140.73    
Median ITL (ms):                         69.79     
P99 ITL (ms):                            652.42    
==================================================
2024-10-03 23:40:57 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:56:00 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:19:02 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:30:02 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:30:51 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40952     
Benchmark duration (s):                  5903.08   
Total input tokens:                      9099002   
Total generated tokens:                  7575036   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.40   
Output token throughput (tok/s):         1283.24   
---------------Time to First Token----------------
Mean TTFT (ms):                          309.71    
Median TTFT (ms):                        270.01    
P99 TTFT (ms):                           821.50    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          144.77    
Median TPOT (ms):                        142.37    
P99 TPOT (ms):                           246.42    
---------------Inter-token Latency----------------
Mean ITL (ms):                           139.27    
Median ITL (ms):                         70.68     
P99 ITL (ms):                            629.27    
==================================================
2024-10-03 23:36:56 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:57:33 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:09:12 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:16:01 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:16:47 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:30:18 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:50:01 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40945     
Benchmark duration (s):                  5903.15   
Total input tokens:                      9097645   
Total generated tokens:                  7573070   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.15   
Output token throughput (tok/s):         1282.89   
---------------Time to First Token----------------
Mean TTFT (ms):                          416.02    
Median TTFT (ms):                        302.70    
P99 TTFT (ms):                           3497.45   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          152.60    
Median TPOT (ms):                        148.90    
P99 TPOT (ms):                           258.18    
---------------Inter-token Latency----------------
Mean ITL (ms):                           147.06    
Median ITL (ms):                         73.59     
P99 ITL (ms):                            665.86    
==================================================
2024-10-03 23:33:16 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:35:00 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:42:28 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:42:51 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:51:30 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:51:43 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:58:30 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:05:34 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:17:53 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:34:19 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:44:44 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40942     
Benchmark duration (s):                  5903.74   
Total input tokens:                      9096117   
Total generated tokens:                  7570361   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.74   
Output token throughput (tok/s):         1282.30   
---------------Time to First Token----------------
Mean TTFT (ms):                          435.29    
Median TTFT (ms):                        400.29    
P99 TTFT (ms):                           1110.35   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          149.13    
Median TPOT (ms):                        145.29    
P99 TPOT (ms):                           264.70    
---------------Inter-token Latency----------------
Mean ITL (ms):                           141.55    
Median ITL (ms):                         70.97     
P99 ITL (ms):                            666.51    
==================================================
2024-10-03 23:27:44 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:38:03 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:43:58 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:44:55 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:47:13 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:54:24 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:54:29 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:55:53 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:58:20 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:18:38 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:29:07 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:30:57 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:34:07 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:38:04 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:50:29 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40937     
Benchmark duration (s):                  5904.60   
Total input tokens:                      9094018   
Total generated tokens:                  7574026   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.16   
Output token throughput (tok/s):         1282.73   
---------------Time to First Token----------------
Mean TTFT (ms):                          519.48    
Median TTFT (ms):                        455.07    
P99 TTFT (ms):                           2435.93   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          156.70    
Median TPOT (ms):                        151.15    
P99 TPOT (ms):                           271.99    
---------------Inter-token Latency----------------
Mean ITL (ms):                           148.79    
Median ITL (ms):                         72.68     
P99 ITL (ms):                            696.21    
==================================================
2024-10-03 23:33:52 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:46:55 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:51:04 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 23:51:31 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:29:06 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:29:58 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:45:11 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-04 00:48:23 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40947     
Benchmark duration (s):                  5912.62   
Total input tokens:                      9096191   
Total generated tokens:                  7567244   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1538.44   
Output token throughput (tok/s):         1279.85   
---------------Time to First Token----------------
Mean TTFT (ms):                          313.25    
Median TTFT (ms):                        267.76    
P99 TTFT (ms):                           827.59    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          143.68    
Median TPOT (ms):                        141.72    
P99 TPOT (ms):                           247.29    
---------------Inter-token Latency----------------
Mean ITL (ms):                           138.52    
Median ITL (ms):                         70.18     
P99 ITL (ms):                            631.86    
==================================================
