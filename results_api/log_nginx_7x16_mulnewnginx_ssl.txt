WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
WARNING 10-03 14:48:54 _custom_ops.py:18] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
Namespace(backend='vllm', base_url='https://vgpu-test-15.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-8.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-9.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-13.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-10.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-6.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-14.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-2.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-5.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-1.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-4.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-12.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-7.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-3.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-16.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
Namespace(backend='vllm', base_url='https://vgpu-test-11.service-inference.ai', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='./ShareGPT_V3_unfiltered_cleaned_split.json', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40960, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=7.0, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: 7.0
2024-10-03 14:56:38 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:03:29 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:03:40 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:03:50 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:00 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:25:47 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:26:48 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:40:50 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:41:34 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:45:53 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:51:40 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:51:45 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:56:46 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:11:51 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:18:52 ERROR on: https://vgpu-test-7.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40934     
Benchmark duration (s):                  5900.44   
Total input tokens:                      9093815   
Total generated tokens:                  7573576   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.21   
Output token throughput (tok/s):         1283.56   
---------------Time to First Token----------------
Mean TTFT (ms):                          6408.13   
Median TTFT (ms):                        7829.98   
P99 TTFT (ms):                           11443.80  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.99     
Median TPOT (ms):                        74.88     
P99 TPOT (ms):                           139.33    
---------------Inter-token Latency----------------
Mean ITL (ms):                           105.76    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7183.75   
==================================================
2024-10-03 15:05:07 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:10:51 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:11:40 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:17:00 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:22:38 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:24:33 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:47:00 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:49:42 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:54:04 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:55:43 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:56:32 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:08:19 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:19:42 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:24:39 ERROR on: https://vgpu-test-1.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40927     
Benchmark duration (s):                  5901.03   
Total input tokens:                      9092913   
Total generated tokens:                  7562207   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1540.90   
Output token throughput (tok/s):         1281.51   
---------------Time to First Token----------------
Mean TTFT (ms):                          6586.58   
Median TTFT (ms):                        8091.61   
P99 TTFT (ms):                           11653.19  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          62.83     
Median TPOT (ms):                        78.10     
P99 TPOT (ms):                           141.97    
---------------Inter-token Latency----------------
Mean ITL (ms):                           109.04    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7436.64   
==================================================
2024-10-03 15:00:01 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:01:05 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:01:24 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:04:06 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:38 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:23:26 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:27:02 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:39:29 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:56:46 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:57:01 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:58:07 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:59:46 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:17:29 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:22:47 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:23:46 ERROR on: https://vgpu-test-4.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40928     
Benchmark duration (s):                  5901.50   
Total input tokens:                      9094572   
Total generated tokens:                  7571021   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1541.06   
Output token throughput (tok/s):         1282.90   
---------------Time to First Token----------------
Mean TTFT (ms):                          6570.52   
Median TTFT (ms):                        7982.63   
P99 TTFT (ms):                           11710.41  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          61.94     
Median TPOT (ms):                        74.03     
P99 TPOT (ms):                           140.47    
---------------Inter-token Latency----------------
Mean ITL (ms):                           107.45    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7151.49   
==================================================
2024-10-03 14:59:39 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:01:59 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:02:24 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:03:41 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:10:02 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:13:17 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:45 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:21:16 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:25:35 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:27:34 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:30:58 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:31:29 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:31:42 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:41:21 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:47:06 ERROR on: https://vgpu-test-3.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40937     
Benchmark duration (s):                  5902.39   
Total input tokens:                      9093416   
Total generated tokens:                  7571858   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1540.63   
Output token throughput (tok/s):         1282.85   
---------------Time to First Token----------------
Mean TTFT (ms):                          6352.50   
Median TTFT (ms):                        7783.19   
P99 TTFT (ms):                           11187.14  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.48     
Median TPOT (ms):                        74.10     
P99 TPOT (ms):                           137.00    
---------------Inter-token Latency----------------
Mean ITL (ms):                           104.87    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7047.77   
==================================================
2024-10-03 15:08:18 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:09:30 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:19:21 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:57 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:21:19 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:31:04 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:34:37 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:37:21 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:41:12 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:43:20 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:48:47 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:50:33 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:51:00 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:57:08 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:03:32 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:12:05 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:12:06 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:15:11 ERROR on: https://vgpu-test-5.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40937     
Benchmark duration (s):                  5902.93   
Total input tokens:                      9094181   
Total generated tokens:                  7569754   
Request throughput (req/s):              6.94      
Input token throughput (tok/s):          1540.62   
Output token throughput (tok/s):         1282.37   
---------------Time to First Token----------------
Mean TTFT (ms):                          6583.60   
Median TTFT (ms):                        7969.37   
P99 TTFT (ms):                           11911.00  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          61.98     
Median TPOT (ms):                        74.41     
P99 TPOT (ms):                           142.05    
---------------Inter-token Latency----------------
Mean ITL (ms):                           107.53    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7106.17   
==================================================
2024-10-03 14:51:58 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 14:55:43 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 14:58:36 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:09:58 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:11:15 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:18:18 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:51 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:21:37 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:27:38 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:01:03 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:09:19 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:19:23 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:26:19 ERROR on: https://vgpu-test-15.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40940     
Benchmark duration (s):                  5903.51   
Total input tokens:                      9097648   
Total generated tokens:                  7570401   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1541.06   
Output token throughput (tok/s):         1282.35   
---------------Time to First Token----------------
Mean TTFT (ms):                          6381.81   
Median TTFT (ms):                        7824.93   
P99 TTFT (ms):                           11206.40  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.80     
Median TPOT (ms):                        74.31     
P99 TPOT (ms):                           137.87    
---------------Inter-token Latency----------------
Mean ITL (ms):                           105.49    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7137.95   
==================================================
2024-10-03 15:01:08 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:06:03 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:08:45 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:09:20 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:12:04 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:03 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:25:34 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:32:55 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:35:03 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:57:01 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:58:58 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:09:03 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:11:12 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:13:09 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:27:05 ERROR on: https://vgpu-test-2.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40936     
Benchmark duration (s):                  5902.93   
Total input tokens:                      9095164   
Total generated tokens:                  7572094   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.79   
Output token throughput (tok/s):         1282.77   
---------------Time to First Token----------------
Mean TTFT (ms):                          6532.68   
Median TTFT (ms):                        7957.64   
P99 TTFT (ms):                           11771.53  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          61.51     
Median TPOT (ms):                        76.10     
P99 TPOT (ms):                           139.81    
---------------Inter-token Latency----------------
Mean ITL (ms):                           106.64    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7154.67   
==================================================
2024-10-03 14:59:54 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:01:39 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:04:31 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:06:00 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:07:09 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:17:15 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:30:34 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:37:48 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:38:19 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:42:59 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:46:51 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:58:21 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:59:16 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:08:40 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:13:20 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:20:15 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:21:54 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:23:23 ERROR on: https://vgpu-test-10.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40931     
Benchmark duration (s):                  5903.31   
Total input tokens:                      9093612   
Total generated tokens:                  7565933   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.43   
Output token throughput (tok/s):         1281.64   
---------------Time to First Token----------------
Mean TTFT (ms):                          6475.17   
Median TTFT (ms):                        7865.77   
P99 TTFT (ms):                           11518.24  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.36     
Median TPOT (ms):                        75.08     
P99 TPOT (ms):                           136.69    
---------------Inter-token Latency----------------
Mean ITL (ms):                           104.65    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7142.76   
==================================================
2024-10-03 14:51:36 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 14:52:12 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:00:28 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:34 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:26:32 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:32:47 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:03:33 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:06:42 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:09:13 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:09:35 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:13:31 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:13:38 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:15:29 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:18:29 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:22:58 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:24:16 ERROR on: https://vgpu-test-9.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40932     
Benchmark duration (s):                  5903.65   
Total input tokens:                      9094882   
Total generated tokens:                  7565748   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.55   
Output token throughput (tok/s):         1281.54   
---------------Time to First Token----------------
Mean TTFT (ms):                          6444.91   
Median TTFT (ms):                        7781.05   
P99 TTFT (ms):                           11781.75  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          59.89     
Median TPOT (ms):                        72.68     
P99 TPOT (ms):                           139.22    
---------------Inter-token Latency----------------
Mean ITL (ms):                           103.88    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            6921.98   
==================================================
2024-10-03 15:06:20 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:08:33 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:12:50 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:19:55 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:35:21 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:38:30 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:58:02 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:10:13 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:12:13 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:15:20 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:16:30 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:26:53 ERROR on: https://vgpu-test-13.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40937     
Benchmark duration (s):                  5904.74   
Total input tokens:                      9096341   
Total generated tokens:                  7569510   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.52   
Output token throughput (tok/s):         1281.94   
---------------Time to First Token----------------
Mean TTFT (ms):                          6500.26   
Median TTFT (ms):                        7996.54   
P99 TTFT (ms):                           11404.12  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          62.03     
Median TPOT (ms):                        77.23     
P99 TPOT (ms):                           139.96    
---------------Inter-token Latency----------------
Mean ITL (ms):                           107.67    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7411.28   
==================================================
2024-10-03 14:56:23 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 14:59:21 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:04:43 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:05:14 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:10:13 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:17:47 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:46 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:22:29 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:25:40 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:28:19 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:35:41 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:36:04 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:37:26 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:48:59 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:51:34 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:51:39 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:52:39 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:52:51 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:07:51 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:17:16 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:17:29 ERROR on: https://vgpu-test-12.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40928     
Benchmark duration (s):                  5904.36   
Total input tokens:                      9093450   
Total generated tokens:                  7567573   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.13   
Output token throughput (tok/s):         1281.69   
---------------Time to First Token----------------
Mean TTFT (ms):                          6311.53   
Median TTFT (ms):                        7777.16   
P99 TTFT (ms):                           11039.26  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.01     
Median TPOT (ms):                        74.10     
P99 TPOT (ms):                           134.00    
---------------Inter-token Latency----------------
Mean ITL (ms):                           104.07    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7096.33   
==================================================
2024-10-03 14:59:34 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:00:53 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:02:04 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:02:52 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:08:50 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:09:58 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:17:27 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:16 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:34 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:22:55 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:24:50 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:27:58 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:29:27 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:36:36 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:36:58 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:38:13 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:45:04 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:51:12 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:53:28 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:59:00 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:59:27 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:59:58 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:02:24 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:04:10 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:18:31 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:19:29 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:23:57 ERROR on: https://vgpu-test-8.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40919     
Benchmark duration (s):                  5905.73   
Total input tokens:                      9090553   
Total generated tokens:                  7566702   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1539.28   
Output token throughput (tok/s):         1281.25   
---------------Time to First Token----------------
Mean TTFT (ms):                          6712.90   
Median TTFT (ms):                        8234.54   
P99 TTFT (ms):                           11850.00  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          62.96     
Median TPOT (ms):                        78.05     
P99 TPOT (ms):                           142.70    
---------------Inter-token Latency----------------
Mean ITL (ms):                           109.17    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7492.57   
==================================================
2024-10-03 14:55:52 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:02:31 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:03:58 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:05:30 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:18:11 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:24:55 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:28:39 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:30:57 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:33:55 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:37:36 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:37:41 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:38:40 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:41:04 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:59:59 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:05:33 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:15:20 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:15:21 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:16:26 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:21:49 ERROR on: https://vgpu-test-14.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40934     
Benchmark duration (s):                  5905.27   
Total input tokens:                      9094954   
Total generated tokens:                  7572663   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.14   
Output token throughput (tok/s):         1282.36   
---------------Time to First Token----------------
Mean TTFT (ms):                          6409.10   
Median TTFT (ms):                        7768.79   
P99 TTFT (ms):                           11569.46  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          59.98     
Median TPOT (ms):                        73.22     
P99 TPOT (ms):                           138.36    
---------------Inter-token Latency----------------
Mean ITL (ms):                           104.00    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            6957.45   
==================================================
2024-10-03 14:58:31 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 14:58:42 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 14:59:18 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 14:59:39 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:11:10 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:21:12 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:25:23 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:25:40 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:43:16 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:45:39 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:55:58 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:00:11 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:00:48 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:10:41 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:20:26 ERROR on: https://vgpu-test-11.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40938     
Benchmark duration (s):                  5906.50   
Total input tokens:                      9097385   
Total generated tokens:                  7569443   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1540.23   
Output token throughput (tok/s):         1281.54   
---------------Time to First Token----------------
Mean TTFT (ms):                          6837.10   
Median TTFT (ms):                        8377.09   
P99 TTFT (ms):                           11921.64  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          63.49     
Median TPOT (ms):                        79.11     
P99 TPOT (ms):                           143.72    
---------------Inter-token Latency----------------
Mean ITL (ms):                           110.06    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7579.81   
==================================================
2024-10-03 14:58:46 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 14:58:51 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:00:19 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:03:56 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:04:11 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:12:02 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:13:39 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:17:25 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:23:09 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:26:09 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:26:55 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:31:00 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:35:02 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:35:04 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:36:09 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:40:38 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:41:28 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:42:03 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:50:57 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:03:26 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:08:01 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:08:43 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:25:06 ERROR on: https://vgpu-test-6.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40922     
Benchmark duration (s):                  5908.53   
Total input tokens:                      9092975   
Total generated tokens:                  7565618   
Request throughput (req/s):              6.93      
Input token throughput (tok/s):          1538.96   
Output token throughput (tok/s):         1280.46   
---------------Time to First Token----------------
Mean TTFT (ms):                          7018.88   
Median TTFT (ms):                        8726.25   
P99 TTFT (ms):                           12384.95  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          67.06     
Median TPOT (ms):                        84.29     
P99 TPOT (ms):                           149.76    
---------------Inter-token Latency----------------
Mean ITL (ms):                           116.38    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            8206.46   
==================================================
2024-10-03 14:59:29 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:00:49 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:03:28 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:08:24 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:10:24 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:20:17 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:21:39 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:23:33 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:24:52 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:32:51 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:35:46 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:48:18 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:55:32 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 15:56:35 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:12:26 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:13:23 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:17:28 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
2024-10-03 16:21:48 ERROR on: https://vgpu-test-16.service-inference.ai/v1/completions Bad Gateway
============ Serving Benchmark Result ============
Successful requests:                     40930     
Benchmark duration (s):                  5913.71   
Total input tokens:                      9096692   
Total generated tokens:                  7569113   
Request throughput (req/s):              6.92      
Input token throughput (tok/s):          1538.24   
Output token throughput (tok/s):         1279.93   
---------------Time to First Token----------------
Mean TTFT (ms):                          6730.63   
Median TTFT (ms):                        8187.11   
P99 TTFT (ms):                           11876.58  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          62.99     
Median TPOT (ms):                        78.18     
P99 TPOT (ms):                           143.72    
---------------Inter-token Latency----------------
Mean ITL (ms):                           109.18    
Median ITL (ms):                         0.02      
P99 ITL (ms):                            7423.59   
==================================================
